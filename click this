import cv2
import torch
import time
import numpy as np
import matplotlib.pyplot as plt
import os
import json
from skimage.measure import find_contours
from segment_anything import sam_model_registry, SamPredictor # SamPredictor도 여기서 import 필요

# --- Setting (설정) ---
sam_checkpoint = "D:/sam_vit_h_4b8939.pth" # 모델경로 정확하게 맞춰야 함
model_type = "vit_h" # 모델타입도 정확히 일치해야 함.

#엔비디아 gpu면 cuda, 아니면 cpu
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

results_dir = "results"
os.makedirs(results_dir, exist_ok=True)

class SAMInteractiveSegmenter:
    def __init__(self, sam_checkpoint, model_type="vit_h", results_dir="results"):
        self.sam_checkpoint = sam_checkpoint
        self.model_type = model_type
        self.results_dir = results_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        os.makedirs(self.results_dir, exist_ok=True)

        self.predictor = None
        self._load_sam_model()

        self.image = None
        self.image_copy = None
        self.current_display_image = None
        self.input_points = []
        self.input_labels = []
        self.input_boxes = []
        
        # COCO 어노테이션 ID 관리를 위한 카운터 (한 세션 내에서 멋드러지고 간지나게 유지)
        self.global_annotation_id_counter = 0 
        self.image_id_counter = 1 # COCO image_id는 1부터 시작하는 것이 일반적

    def _load_sam_model(self):
        """SAM 모델을 로드합니다."""
        print('Loading Segment Anything Model...')
        try:
            # from segment_anything import sam_model_registry, SamPredictor # 이미 상단에서 import했음음
            sam = sam_model_registry[self.model_type](checkpoint=self.sam_checkpoint)
            sam.to(device=self.device)
            self.predictor = SamPredictor(sam)
            print("SAM model loaded successfully.")
        except ImportError:
            print("Error: 'segment_anything' library not found. Please install it.")
            print("You can install it via pip: pip install segment_anything")
            self.predictor = None
        except Exception as e:
            print(f"Error loading SAM model: {e}")
            print("Please ensure 'sam_checkpoint' path is correct and 'model_type' matches the checkpoint.")
            self.predictor = None

    def _show_mask(self, mask, ax, random_color=False):
        """마스크를 시각화합니다."""
        if random_color:
            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
        else:
            color = np.array([30/255, 144/255, 255/255, 0.6])
        h, w = mask.shape[-2:]
        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
        ax.imshow(mask_image)

    def _show_points(self, coords, labels, ax, marker_size=375):
        """포인트를 시각화합니다."""
        pos_points = coords[labels==1]
        neg_points = coords[labels==0]
        ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
        ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)

    def _show_box(self, box, ax):
        """바운딩 박스를 시각화합니다."""
        x0, y0 = box[0], box[1]
        w, h = box[2] - box[0], box[3] - box[1]
        ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))

    def _extract_coco_polygon_from_mask(self, mask):
        """
        마스크에서 COCO 형식의 폴리곤 좌표를 추출합니다.
        COCO 형식: [[x1, y1, x2, y2, ...]] (flattened list)
        """
        contours = find_contours(mask, 0.5) # 0.5는 이진 마스크의 임계값
        coco_polygons = []
        for contour in contours:
            segmentation = np.flip(contour, axis=1).ravel().tolist()
            if len(segmentation) >= 6:
                coco_polygons.append(segmentation)
        return coco_polygons

    def _calculate_bbox_from_mask(self, mask):
        """마스크에서 COCO 형식의 바운딩 박스 [x, y, width, height]를 계산합니다."""
        # 마스크에서 True인 픽셀의 인덱스를 찾음
        rows = np.any(mask, axis=1)
        cols = np.any(mask, axis=0)
        
        ymin, ymax = np.where(rows)[0][[0, -1]] if np.any(rows) else (0, 0)
        xmin, xmax = np.where(cols)[0][[0, -1]] if np.any(cols) else (0, 0)
        
        # COCO bbox: [x, y, width, height]
        bbox = [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)]
        return bbox

    def _calculate_area_from_mask(self, mask):
        """마스크에서 면적을 계산합니다."""
        return float(np.sum(mask))

    def _draw_point_and_box(self, event, x, y, flags, param):
        """마우스 콜백 함수입니다."""
        display_img_for_drawing = self.current_display_image.copy()

        if self.point_mode:
            if event == cv2.EVENT_LBUTTONDOWN:
                cv2.circle(display_img_for_drawing, (x, y), 5, (0, 255, 0), -1)
                self.input_points.append([x, y])
                self.input_labels.append(1)
            elif event == cv2.EVENT_RBUTTONDOWN:
                cv2.circle(display_img_for_drawing, (x, y), 5, (0, 0, 255), -1)
                self.input_points.append([x, y])
                self.input_labels.append(0)
            cv2.imshow('Image', display_img_for_drawing)
            self.current_display_image = display_img_for_drawing

        elif self.drawing_box:
            if event == cv2.EVENT_LBUTTONDOWN:
                self.ix, self.iy = x, y
                self.drawing = True
            elif event == cv2.EVENT_MOUSEMOVE:
                if self.drawing:
                    temp_image_for_drag = self.current_display_image.copy()
                    cv2.rectangle(temp_image_for_drag, (self.ix, self.iy), (x, y), (0, 255, 0), 1)
                    cv2.imshow('Image', temp_image_for_drag)
            elif event == cv2.EVENT_LBUTTONUP:
                self.drawing = False
                cv2.rectangle(display_img_for_drawing, (self.ix, self.iy), (x, y), (0, 255, 0), 1)
                self.input_boxes.append([self.ix, self.iy, x, y])
                cv2.imshow('Image', display_img_for_drawing)
                self.current_display_image = display_img_for_drawing

    def _reset_inputs(self):
        """입력(점, 박스)을 초기화하고 이미지를 원본으로 복원합니다."""
        self.input_points = []
        self.input_labels = []
        self.input_boxes = []
        self.drawing_box = False
        self.point_mode = False
        self.current_display_image = self.image_copy.copy()
        cv2.imshow('Image', self.current_display_image)

    def _save_coco_result(self, img_path, masks, input_data):
        """
        세그멘테이션 결과를 COCO 형식의 JSON 파일과 시각화 이미지로 저장합니다.
        단일 이미지에 대한 COCO 형식 JSON을 생성합니다.
        """
        current_time = time.strftime("%Y%m%d_%H%M%S") # YYYYMMDD_HHMMSS 형식
        filename = os.path.basename(img_path)
        img_height, img_width = self.image_copy.shape[:2]

        coco_data = {
            "info": {
                "description": "Interactive SAM Segmentation Result",
                "version": "1.0",
                "date_created": current_time,
                "contributor": "SAMInteractiveSegmenter"
            },
            "licenses": [],
            "images": [],
            "annotations": [],
            "categories": [{"id": 1, "name": "object", "supercategory": "none"}] # 기본 카테고리 ID를 1로 설정
        }

        image_info = {
            "id": self.image_id_counter,
            "width": img_width,
            "height": img_height,
            "file_name": filename,
            "license": 0, # 라이선스 ID있으면 넣기기
            "date_captured": current_time
        }
        coco_data["images"].append(image_info)

        # 마스크를 COCO 어노테이션으로 변환
        plt.figure(num='Segmentation Result', figsize=(10, 10))
        plt.imshow(cv2.cvtColor(self.image_copy, cv2.COLOR_BGR2RGB))
        ax = plt.gca()

        for i, mask in enumerate(masks):
            mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else mask
            self._show_mask(mask_np, ax, random_color=True) # 각 마스크마다 다른 색상 (다중 객체 시각화)

            # COCO 형식 폴리곤 추출
            coco_segmentation = self._extract_coco_polygon_from_mask(mask_np)
            
            if not coco_segmentation: # 유효한 폴리곤이 없으면 건너뛰어버리기기
                continue

            # COCO 형식 bbox 계산 (마스크로부터 직접 계산)
            coco_bbox = self._calculate_bbox_from_mask(mask_np)
            coco_area = self._calculate_area_from_mask(mask_np)

            annotation = {
                "id": self.global_annotation_id_counter,
                "image_id": self.image_id_counter,
                "category_id": 1, # 'object' 카테고리 ID
                "segmentation": coco_segmentation, # [[x1, y1, x2, y2, ...]]
                "area": coco_area,
                "bbox": coco_bbox, # [x, y, width, height]
                "iscrowd": 0 # 0 for individual objects, 1 for crowd (RLE)
            }
            coco_data["annotations"].append(annotation)
            self.global_annotation_id_counter += 1

        # 원본 입력 프롬프트 시각화할지 말지 아래에서 선택하면ㄷ ㅗㅣㅁ.
        if input_data:
            if input_data.get('input_boxes') and len(input_data['input_boxes']) > 0:
                for box_coords in input_data['input_boxes']:
                    self._show_box(box_coords, ax)
            if input_data.get('input_points') and len(input_data['input_points']) > 0:
                self._show_points(np.array(input_data['input_points']), np.array(input_data['input_labels']), ax)

        plt.axis('off')
        
        # 이미지 저장
        image_save_name = os.path.join(self.results_dir, f"{os.path.splitext(filename)[0]}_masked_{current_time}.png")
        plt.savefig(image_save_name, bbox_inches='tight', pad_inches=0)
        plt.show() # 이미지 표시
        print(f"Masked image saved to {image_save_name}")

        # JSON 파일 저장
        json_file_name = os.path.join(self.results_dir, f"{os.path.splitext(filename)[0]}_coco_{current_time}.json")
        with open(json_file_name, 'w', encoding='utf-8') as f:
            json.dump(coco_data, f, indent=4, ensure_ascii=False)
        print(f"COCO JSON result saved to {json_file_name}")

        self.image_id_counter += 1 # 다음 이미지를 위해 ID 증가 (현재는 단일 이미지 처리지만, 확장성 고려)


    def run_single_object_segmentation(self):
        """단일 객체 세그멘테이션 모드를 실행합니다."""
        print('---Single Object Segmentation Mode---')
        print('Support \n 1. Multiple points \n 2. Single bbox\n')
        print('press the keyboard')
        print('p: Point Mode / b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')

        while True:
            key2 = cv2.waitKey(1)

            if key2 & 0xFF == ord('q'):
                print('---Segmentation Mode Change---')
                print('s: Single Object Segmentation / m: Multiple Object Segmentation / q: Quit\n')
                self._reset_inputs()
                break
            elif key2 & 0xFF == ord('p'):
                self.point_mode = True
                self.drawing_box = False
                print('Now: Point Mode')
                print('p: Point Mode / b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')
            elif key2 & 0xFF == ord('b'):
                self.drawing_box = True
                self.point_mode = False
                print('Now: Bbox Mode')
                print('p: Point Mode / b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')
            elif key2 & 0xFF == ord('r'):
                self._reset_inputs()
                print('Now: Reset all points and boxes')
                print('p: Point Mode / b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')
            elif key2 & 0xFF == ord('i'):
                print('Now: Model Inference')
                print('p: Point Mode / b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')

                points_for_sam = np.array(self.input_points) if self.input_points else None
                labels_for_sam = np.array(self.input_labels) if self.input_labels else None
                boxes_for_sam = np.array(self.input_boxes) if self.input_boxes else None

                if boxes_for_sam is not None and boxes_for_sam.shape[0] > 1:
                    print('Error: In Single Object Segmentation mode, only one bbox is allowed.')
                    continue

                is_empty_points = points_for_sam is None or points_for_sam.size == 0
                is_empty_boxes = boxes_for_sam is None or boxes_for_sam.size == 0

                if is_empty_boxes and is_empty_points:
                    print('Error: No Input Prompt (points or box) provided.')
                    continue
                
                # 마스크 예측
                masks, scores, logits = self.predictor.predict(
                    point_coords=points_for_sam,
                    point_labels=labels_for_sam,
                    box=boxes_for_sam[0] if boxes_for_sam is not None else None,
                    multimask_output=False # 단일 객체는 가장 적합한 마스크 1개만
                )
                
                input_data_for_json = {
                    "input_points": self.input_points,
                    "input_labels": self.input_labels,
                    "input_boxes": self.input_boxes
                }
                # _save_coco_result 함수 호출
                self._save_coco_result(self.img_path, masks, input_data_for_json)
                self._reset_inputs() # 결과 저장 후 입력 초기화

    def run_multiple_object_segmentation(self):
        """다중 객체 세그멘테이션 모드를 실행합니다."""
        print('---Multiple Object Segmentation---')
        print('Support \n 1. Multiple bboxes \n')
        print('press the keyboard')
        print('b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')

        while True:
            key2 = cv2.waitKey(1)

            if key2 & 0xFF == ord('q'):
                print('---Segmentation Mode Change---')
                print('s: Single Object Segmentation / m: Multiple Object Segmentation / q: Quit\n')
                self._reset_inputs()
                break
            elif key2 & 0xFF == ord('b'):
                self.drawing_box = True
                self.point_mode = False
                print('Now: Bbox Mode')
                print('b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')
            elif key2 & 0xFF == ord('r'):
                self._reset_inputs()
                print('Now: Reset all points and boxes')
                print('b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')
            elif key2 & 0xFF == ord('i'):
                print('Now: Model Inference')
                print('b: Bbox Mode / r: Reset / i: Model Inference / q: Quit\n')

                if not self.input_boxes:
                    print('Error: No Input Prompt (box) provided.')
                    continue

                input_boxes_sam = torch.tensor(self.input_boxes, device=self.device)
                transformed_boxes = self.predictor.transform.apply_boxes_torch(input_boxes_sam, self.image.shape[:2])

                masks, scores, logits = self.predictor.predict_torch( # scores와 logits 사용하지 않아도 되지만, 반환 값 매칭
                    point_coords=None,
                    point_labels=None,
                    boxes=transformed_boxes,
                    multimask_output=True # 다중 마스크 출력을 통해 여러 객체 마스크 얻음
                )
                
                # masks는 (num_boxes, num_masks_per_box, H, W) 형태일 수 있습니다.
                # 여기서는 각 박스에 대해 가장 높은 점수를 가진 마스크(첫 번째)를 선택하여 처리합니다.
                processed_masks = masks[:, 0, :, :] # 각 박스에 대한 첫 번째 마스크만 선택 (N, H, W)

                input_data_for_json = {
                    "input_boxes": self.input_boxes
                }
                # _save_coco_result 함수 호출
                self._save_coco_result(self.img_path, processed_masks, input_data_for_json)
                self._reset_inputs() # 결과 저장 후 입력 초기화

    def run(self):
        """메인 실행 루프입니다."""
        if self.predictor is None:
            print("SAM model failed to load. Exiting.")
            return

        while True:
            self.img_path = input("Image path: ")
            print()

            self.image = cv2.imread(self.img_path)
            if self.image is None:
                print('Error: No such file or directory.\n')
                continue

            self.image_copy = self.image.copy()
            self.current_display_image = self.image.copy()
            self._reset_inputs() # 모든 입력 및 디스플레이 초기화 (이전 이미지의 데이터 초기화)

            # SAM predictor에 이미지 설정 (RGB 변환 필요)
            self.predictor.set_image(cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB))

            print('------')
            print('Segmentation Mode List')
            print('s: Single Object Segmentation / m: Multiple Object Segmentation / q: Quit')
            print('------\n')

            cv2.imshow('Image', self.current_display_image)
            cv2.setMouseCallback('Image', self._draw_point_and_box)

            while True:
                key = cv2.waitKey(1)

                if key & 0xFF == ord('q'):
                    break # 메인 루프 (이미지 선택)로 돌아가기
                elif key & 0xFF == ord('s'):
                    self.run_single_object_segmentation()
                    cv2.imshow('Image', self.current_display_image) # 모드 종료 후 현재 이미지 다시 표시
                elif key & 0xFF == ord('m'):
                    self.run_multiple_object_segmentation()
                    cv2.imshow('Image', self.current_display_image) # 모드 종료 후 현재 이미지 다시 표시

            cv2.destroyAllWindows() # 현재 이미지 창 닫기

            is_continue = input("Continue with new image (y/n): ")
            if is_continue.lower() == 'y':
                print()
                continue
            else:
                print('---Exit---')
                break

if __name__ == '__main__':
    segmenter = SAMInteractiveSegmenter(
        sam_checkpoint="D:/sam_vit_h_4b8939.pth", # 본인의 SAM 모델 경로로 변경
        model_type="vit_h",
        results_dir="results"
    )
    segmenter.run()
